{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: detected 160 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 160 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "from data_partitioner import *\n",
    "from parallel_tagging import *\n",
    "from biberplus.tagger import load_config\n",
    "from biberplus.tagger.tagger_utils import load_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"RC_2020-05.gz\"\n",
    "\n",
    "input_path = os.path.join('/shared/4/datasets/long-reddit/', file_name)\n",
    "partition_dir = os.path.join('/shared/3/projects/hiatus/tagged_data/partitions/reddit', file_name.split('.')[0]) \n",
    "tag_dir = os.path.join('/shared/3/projects/hiatus/tagged_data/partitions/tagged-reddit', file_name.split('.')[0])\n",
    "output_dir= '/shared/3/projects/hiatus/tagged_data/long-reddit/'\n",
    "    \n",
    "# ensures all directories will exist\n",
    "for directory in [partition_dir, tag_dir, output_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# initialize author subreddit dictionary\n",
    "author_subreddit_counts = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the file into 100 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "partition_file(input_path, partition_dir, chunks=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag each partition with 1 CPU \n",
    "\n",
    "Set nice value low so we don't hog the server\n",
    "\n",
    "**Tagger config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "config.update({'use_gpu': False, \n",
    "               'biber': True,\n",
    "               'binary_tags': True, \n",
    "               'function_words': True,\n",
    "               'token_normalization': 100})\n",
    "tokenizer = load_tokenizer(use_gpu=False)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {13651: 38, 12000: 16, 13000: 44, 13632: 1})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "tag_partitions(config,\n",
    "               input_directory=partition_dir,\n",
    "               output_directory=tag_dir,\n",
    "               num_workers=101,\n",
    "               post_counts=author_subreddit_counts,\n",
    "               default_niceness=0\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of authors: 2011384\n",
      "Total number of posts: 10666573\n",
      "Average number of posts per author: 5.30\n",
      "Total number of authors with over 5 total posts: 283038\n",
      "Total number of authors with over 10 total posts: 142742\n",
      "Total number of unique subreddits: 71749\n"
     ]
    }
   ],
   "source": [
    "# utilities:\n",
    "\n",
    "# Counts the number of tagged entries across each partition\n",
    "# from collections import defaultdict\n",
    "# counts = defaultdict(int)\n",
    "# for i in range(1, 103):\n",
    "#     try:\n",
    "#         counts[count_lines(tag_dir + f\"/partition-{i}.jsonl-tagged.gz\")] += 1\n",
    "#     except Exception:\n",
    "#         continue\n",
    "# print(counts)\n",
    "\n",
    "def print_stats(post_counts):\n",
    "    total_posts = 0\n",
    "    authors_over_5 = 0\n",
    "    authors_over_10 = 0\n",
    "    unique_subs = set()\n",
    "\n",
    "    for author, subs in post_counts.items():\n",
    "        author_posts = sum(subs.values())\n",
    "        total_posts += author_posts\n",
    "        unique_subs.update(subs.keys())\n",
    "        \n",
    "        if author_posts > 5:\n",
    "            authors_over_5 += 1\n",
    "        if author_posts > 10:\n",
    "            authors_over_10 += 1\n",
    "\n",
    "    total_authors = len(post_counts)\n",
    "    avg_posts = float(total_posts) / total_authors if total_authors else 0.0\n",
    "\n",
    "    print(f\"Total number of authors: {total_authors}\")\n",
    "    print(f\"Total number of posts: {total_posts}\")\n",
    "    print(f\"Average number of posts per author: {avg_posts:.2f}\")\n",
    "    print(f\"Total number of authors with over 5 total posts: {authors_over_5}\")\n",
    "    print(f\"Total number of authors with over 10 total posts: {authors_over_10}\")\n",
    "    print(f\"Total number of unique subreddits: {len(unique_subs)}\")\n",
    "    \n",
    "# Individual partition\n",
    "# print_stats(author_subreddit_counts) \n",
    "\n",
    "#All partitions\n",
    "# import csv\n",
    "# import os\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def read_tsv_to_dict(file_path):\n",
    "#     # Function to read a single TSV and update the main dictionary\n",
    "#     with open(file_path, 'r', newline='') as f:\n",
    "#         reader = csv.DictReader(f, delimiter='\\t')\n",
    "#         for row in reader:\n",
    "#             author = row['author']\n",
    "#             subreddit = row['subreddit']\n",
    "#             post_count = int(row['post_count'])\n",
    "#             # If author exists in the dictionary, update their count for the subreddit\n",
    "#             if author in aggregated_data:\n",
    "#                 if subreddit in aggregated_data[author]:\n",
    "#                     aggregated_data[author][subreddit] += post_count\n",
    "#                 else:\n",
    "#                     aggregated_data[author][subreddit] = post_count\n",
    "#             else:\n",
    "#                 # If author does not exist, create new nested dictionary\n",
    "#                 aggregated_data[author] = {subreddit: post_count}\n",
    "\n",
    "# def process_directory(directory):\n",
    "#     # Function to process each file in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.tsv'):\n",
    "#             file_path = os.path.join(directory, filename)\n",
    "#             read_tsv_to_dict(file_path)\n",
    "\n",
    "# # Main dictionary to hold all data\n",
    "# aggregated_data = defaultdict(dict)\n",
    "\n",
    "# # Path to the directory containing the TSV files\n",
    "# directory_path = '/shared/3/projects/hiatus/tagged_data/long-reddit'\n",
    "# process_directory(directory_path)\n",
    "\n",
    "# # Now aggregated_data contains all the combined data\n",
    "# print_stats(aggregated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output_tsv = output_dir + file_name.split('.')[0] + '-counts.tsv'\n",
    "# Write the post counts to a TSV file\n",
    "write_to_tsv(author_subreddit_counts, output_tsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the partitioned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_tagged_files(input_directory=tag_dir,\n",
    "                  output_file=os.path.join(output_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the temp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_partitioned_files(partition_dir)\n",
    "delete_partitioned_files(tag_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
