{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc291c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 06:01:27.825455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 06:01:29,848] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.custom_training.model_utils import load_model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be107500",
   "metadata": {},
   "source": [
    "## Load the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f88de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in roberta-base tokenizer\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/shared/3/projects/hiatus/models/pan20/roberta-test/last/'\n",
    "# model_dir = 'roberta-base'\n",
    "pretrained_model = 'roberta-base'\n",
    "\n",
    "def load_tokenizer(pretrained_model):\n",
    "    print(f\"Loading in {pretrained_model} tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(model_dir)\n",
    "model = model.to('cuda')\n",
    "tokenizer = load_tokenizer(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47ccbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42098 5263 5262\n"
     ]
    }
   ],
   "source": [
    "input_directory = '/shared/3/datasets/PAN/pan20-av-training-small/'\n",
    "\n",
    "train = pd.read_json(input_directory + 'train.jsonl', lines=True)\n",
    "dev = pd.read_json(input_directory + 'dev.jsonl', lines=True)\n",
    "test = pd.read_json(input_directory + 'test.jsonl', lines=True)\n",
    "print(len(train), len(dev), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafc4b1",
   "metadata": {},
   "source": [
    "## Setup dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5612ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextCollator(DataCollatorForLanguageModeling):\n",
    "    tokenizer: PreTrainedTokenizerFast\n",
    "    padding: Union[bool, str] = True\n",
    "    return_attention_mask: Optional[bool] = True\n",
    "    max_length: Optional[int] = 350\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]):\n",
    "        text_one_sents = self._encode_text(features, 0)\n",
    "        text_two_sents = self._encode_text(features, 1)\n",
    "\n",
    "        batchA = self._prepare_batch(text_one_sents)\n",
    "        batchB = self._prepare_batch(text_two_sents)\n",
    "        \n",
    "        labels = [feature['same'] for feature in features]\n",
    "\n",
    "        return batchA, batchB, labels\n",
    "\n",
    "    def _encode_text(self, features, index):\n",
    "        return [{'input_ids': self.tokenizer(feature['pair'][index])['input_ids'][:self.max_length]} for feature in\n",
    "                features]\n",
    "\n",
    "    def _prepare_batch(self, sents):\n",
    "        return self.tokenizer.pad(\n",
    "            sents,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=self.return_attention_mask,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "collator = TextCollator(tokenizer=tokenizer, max_length=350)\n",
    "train_dataset = Dataset.from_pandas(train).shuffle()\n",
    "test_dataset = Dataset.from_pandas(test).shuffle()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,  \n",
    "    shuffle=False,\n",
    "    collate_fn=collator \n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,  \n",
    "    shuffle=False,\n",
    "    collate_fn=collator \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52318b",
   "metadata": {},
   "source": [
    "## Build the embeddings for the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a89dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                            | 0/10525 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5093 > 512). Running this sequence through the model will result in indexing errors\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/anaconda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2416: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10525/10525 [30:44<00:00,  5.71it/s]\n"
     ]
    }
   ],
   "source": [
    "out_directory = '/shared/3/datasets/PAN/pan20-av-training-small/'\n",
    "train_file = os.path.join(out_directory, 'pan_small_roberta_train_embeddings_zero.parquet.gzip')\n",
    "\n",
    "def get_cls(outputs):\n",
    "    last_hidden_state = outputs['hidden_states'][-1]\n",
    "    cls_representation = last_hidden_state[:, 0, :]\n",
    "    return cls_representation\n",
    "\n",
    "def extract_and_store_data(data_loader, model, device, parquet_file):\n",
    "    output_data = []\n",
    "\n",
    "    for _, (batchA, batchB, labels) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        batchA, batchB = batchA.to(device), batchB.to(device)\n",
    "\n",
    "        # Get styles from model output\n",
    "        outputsA = model(**batchA, output_hidden_states=True)\n",
    "        cls_A = get_cls(outputsA).tolist()\n",
    "        outputsB = model(**batchB, output_hidden_states=True)\n",
    "        cls_B = get_cls(outputsB).tolist()\n",
    "\n",
    "        # Combine labels and embedding information\n",
    "        batch_output = [[label] + sA + sB for label, sA, sB in zip(labels, cls_A, cls_B)]\n",
    "        output_data.extend(batch_output)\n",
    "\n",
    "    # Create dataframe from output_data and save to parquet\n",
    "    cols_a = [f'A{i}' for i in range(768)]\n",
    "    cols_b = [f'B{i}' for i in range(768)]\n",
    "    label_column = ['same']\n",
    "    df_columns = label_column + cols_a + cols_b\n",
    "\n",
    "    df = pd.DataFrame(output_data, columns=df_columns)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    df.to_parquet(parquet_file, compression='gzip')\n",
    "\n",
    "\n",
    "def process_training_data():\n",
    "#     if os.path.isfile(train_file):\n",
    "    if False:\n",
    "        print(\"Loading from file\")\n",
    "        return pd.read_parquet(train_file)\n",
    "    else:\n",
    "        device = 'cuda'\n",
    "        extract_and_store_data(train_dataloader, model, device, train_file)\n",
    "        return pd.read_parquet(train_file)\n",
    "\n",
    "train_style = process_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2e8e8",
   "metadata": {},
   "source": [
    "**Repeat for the testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb0f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building test file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/1316 [00:00<?, ?it/s]/opt/anaconda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2416: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1316/1316 [04:03<00:00,  5.40it/s]\n"
     ]
    }
   ],
   "source": [
    "out_directory = '/shared/3/datasets/PAN/pan20-av-training-small/'\n",
    "test_file = os.path.join(out_directory, 'pan_small_roberta_test_embeddings_zero.parquet.gzip')\n",
    "\n",
    "def process_testing_data():\n",
    "#     if os.path.isfile(test_file):\n",
    "    if False:\n",
    "        print(\"Loading from file\")\n",
    "        return pd.read_parquet(test_file)\n",
    "    else:\n",
    "        print(\"Building test file\")\n",
    "        device = 'cuda'\n",
    "        extract_and_store_data(test_dataloader, model, device, test_file)\n",
    "        return pd.read_parquet(test_file)\n",
    "\n",
    "test_style = process_testing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a9225",
   "metadata": {},
   "source": [
    "## Train a Random Forest model and run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad15ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6742683390345876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.64      0.71      0.67      2498\n",
      "        True       0.71      0.64      0.67      2764\n",
      "\n",
      "    accuracy                           0.67      5262\n",
      "   macro avg       0.68      0.68      0.67      5262\n",
      "weighted avg       0.68      0.67      0.67      5262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = train_style['same']\n",
    "X_train = train_style.drop('same', axis=1)\n",
    "\n",
    "y_test = test_style['same']\n",
    "X_test = test_style.drop('same', axis=1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eeddcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7673888255416191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.70      0.74      2498\n",
      "        True       0.75      0.83      0.79      2764\n",
      "\n",
      "    accuracy                           0.77      5262\n",
      "   macro avg       0.77      0.76      0.76      5262\n",
      "weighted avg       0.77      0.77      0.77      5262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = train_style['same']\n",
    "X_train = train_style.drop('same', axis=1)\n",
    "\n",
    "y_test = test_style['same']\n",
    "X_test = test_style.drop('same', axis=1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99c9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7803116685670848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.71      0.75      2498\n",
      "        True       0.76      0.85      0.80      2764\n",
      "\n",
      "    accuracy                           0.78      5262\n",
      "   macro avg       0.78      0.78      0.78      5262\n",
      "weighted avg       0.78      0.78      0.78      5262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = train_style['same']\n",
    "X_train = train_style.drop('same', axis=1)\n",
    "\n",
    "y_test = test_style['same']\n",
    "X_test = test_style.drop('same', axis=1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87af8f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7848726719878373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.70      0.76      2498\n",
      "        True       0.76      0.86      0.81      2764\n",
      "\n",
      "    accuracy                           0.78      5262\n",
      "   macro avg       0.79      0.78      0.78      5262\n",
      "weighted avg       0.79      0.78      0.78      5262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = train_style['same']\n",
    "X_train = train_style.drop('same', axis=1)\n",
    "\n",
    "y_test = test_style['same']\n",
    "X_test = test_style.drop('same', axis=1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
